{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "generative_ai_disabled": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jatingpt/NLP/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#How to remove punctuation and lowercase everything."
      ],
      "metadata": {
        "id": "gBZ937sicaJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #1. Text Cleaning and Preprocessing\n",
        " #2. Tokenization- Word Tokenization and Sentence Tokenization\n",
        " #3. Stopwords Removal\n",
        " #4. Lemmatization and Stemming\n",
        " #5. Named Entity Recognition using spaCy and NLTK\n",
        " #6. POS Tagging\n",
        " #7. TF-IDF Vectorization\n",
        " #8. n-grams\n",
        " #9. Text Classification using Naive Bayes\n",
        " #7. Word Embeddings: Word2Vec (Gensim)\n",
        " #8. Named Entity Recognition using spaCy\n",
        " #9. Sentiment Analysis using TextBlob\n",
        " #10. Text Generation using GPT-2 (Transformers)\n",
        " #11. Topic Modeling with LDA (Gensim)\n",
        " #12. POS Tagging"
      ],
      "metadata": {
        "id": "9nz0wYuqtYUZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution1 -                                     -- Text Cleaning and Preprocessing --\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "texts = [\"Hello!!! This is @Jatin. I love NLP... #awesome\", \"Text-cleaning is crucial :)\"]\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
        "    text = re.sub(r'#[A-Za-z0-9_]+', '', text)  # Remove hashtags\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove punctuation\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "cleaned_texts = [clean_text(t) for t in texts]\n",
        "print(cleaned_texts)"
      ],
      "metadata": {
        "id": "flRCm7IOtYR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 2-                                    -- Tokenization --\n",
        "sentence = \"hey how are you, is everything good?\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "wt = word_tokenize(sentence)\n",
        "wt"
      ],
      "metadata": {
        "id": "wQ80yP3atYPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Hello guys. I I hope you all are fine. We will meet together.\"\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "st = sent_tokenize(sentence)\n",
        "st"
      ],
      "metadata": {
        "id": "L4hW9tf3IE4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 3-                           -- Stopwords Removal --\n",
        "sentence = \"Hey hi, a bag is not enough for these things.\"\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = stopwords.words(\"english\")\n",
        "stop_words\n",
        "\n",
        "sentence = sentence.lower()\n",
        "\n",
        "sentence = sentence.split()\n",
        "\n",
        "for i in sentence:\n",
        "  if i not in stop_words:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "ATQXg0eatYMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 4                                      -- Lemmatization --\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wl = WordNetLemmatizer()\n",
        "wl.lemmatize(\"mice\")"
      ],
      "metadata": {
        "id": "aSqO1y8v0gQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 4 Continued-                       -- Stemming --\n",
        "from nltk.stem import LancasterStemmer, PorterStemmer, RegexpStemmer, SnowballStemmer\n",
        "\n",
        "#Creating a variable for them\n",
        "l = LancasterStemmer()\n",
        "p = PorterStemmer()\n",
        "r = RegexpStemmer('ing')\n",
        "s = SnowballStemmer(\"english\")\n",
        "\n",
        "l.stem(\"Changing\")\n",
        "r.stem('meaning')\n",
        "s.stem('changing')\n",
        "p.stem('learning')"
      ],
      "metadata": {
        "id": "NxaV61asG2W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"Vikas is going to win the race in 2020 in Phulera Dist\"\n",
        "\n",
        "doc = nlp(text)\n",
        "for ent in doc.ents:\n",
        "  print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "vyXv4rZpDhSN",
        "outputId": "7bd44dd6-5fb3-40bb-8109-916f93cabe9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vikas ORG\n",
            "2020 DATE\n",
            "Phulera GPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 5-                               -- NER Using SpaCy --\n",
        "# import spacy\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# text = \"A tree is looking at the human, please don't cut me! I will give you 100$\"\n",
        "# doc = nlp(text)\n",
        "\n",
        "# for ent in doc.ents:\n",
        "#   print(ent.text, ent.label_)\n",
        "\n",
        "\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "3QKjA-3dTFa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 5-                                               -- NER Using NLTK --\n",
        "import nltk\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "# Sample sentence\n",
        "text = \"Barack Obama was the 44th President of the United States. He was born in Hawaii.\"\n",
        "\n",
        "# Step 1: Tokenize the sentence\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Step 2: Part-of-Speech (POS) tagging\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Step 3: Named Entity Recognition (chunking)\n",
        "named_entities = nltk.ne_chunk(pos_tags)\n",
        "\n",
        "# Step 4: Print Named Entities\n",
        "print(\"Named Entities found:\")\n",
        "for chunk in named_entities:\n",
        "    if hasattr(chunk, 'label'):\n",
        "        print(f\"{chunk.label()} : {' '.join(c[0] for c in chunk)}\")"
      ],
      "metadata": {
        "id": "Vj4pjhFXTPcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 6-                                               -- POS Tagging --\n",
        "from nltk.tokenize import word_tokenize\n",
        "sentence = \"Hey ravi, let us go Goa.\"\n",
        "w = word_tokenize(sentence)\n",
        "w\n",
        "\n",
        "from nltk import pos_tag\n",
        "p = pos_tag(w)\n",
        "p"
      ],
      "metadata": {
        "id": "_x6EkYk3Tk8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 7-                             -- TFIDF --\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\"I love NLP and Machine Learning\", \"NLP is a part of artificial intelligence.\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "x = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(x.toarray())"
      ],
      "metadata": {
        "id": "Ofcw2GMg0gN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 6                                    -- Text Classificationn Using Naive Bayes --\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'text': ['I love python', 'NLP is awesome', 'I hate bugs', 'Debugging is painful'],\n",
        "    'label': [1, 1, 0, 0]  # 1: Positive, 0: Negative\n",
        "})\n",
        "\n",
        "x = TfidfVectorizer().fit_transform(df[\"text\"])\n",
        "y = df['label']\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size= 0.25)\n",
        "model = MultinomialNB()\n",
        "model.fit(x_train, y_train)\n",
        "pred = model.predict(x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "metadata": {
        "id": "jzyVrgYk0gKE",
        "outputId": "4b23dcdd-f7d2-4549-c21d-499fa53d9247",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       1.0\n",
            "           1       0.00      0.00      0.00       0.0\n",
            "\n",
            "    accuracy                           0.00       1.0\n",
            "   macro avg       0.00      0.00      0.00       1.0\n",
            "weighted avg       0.00      0.00      0.00       1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 7-                                    -- Word Embeddings(Word2Vec) --\n",
        "!pip install gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Small training data (list of tokenized sentences)\n",
        "sentences = [['I', 'love', 'machine', 'learning'], ['NLP', 'is', 'awesome']]\n",
        "\n",
        "# Create the Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=10, window=2, min_count=1, workers=1)\n",
        "\n",
        "# Print the word vector for 'learning'\n",
        "print(model.wv['learning'])  # A 10-dimensional vector\n",
        "\n",
        "\n",
        "\n",
        "#vector_size=10 → Each word will be represented as a 10-dimensional vector\n",
        "#window=2 → Looks at 2 words before and after the target word\n",
        "#min_count=1 → Even rare words (like those occurring once) will be included\n",
        "#workers=1 → Uses 1 CPU core for training"
      ],
      "metadata": {
        "id": "wpSKWA260gIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 8-                               -- NER Using SpaCy --\n",
        "# import spacy\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# text = \"A tree is looking at the human, please don't cut me! I will give you 100$\"\n",
        "# doc = nlp(text)\n",
        "\n",
        "# for ent in doc.ents:\n",
        "#   print(ent.text, ent.label_)\n",
        "\n",
        "\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "FTny-oVq0fvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 8-                                   -- NER Using NLTK --\n",
        "import nltk\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "# Sample sentence\n",
        "text = \"Barack Obama was the 44th President of the United States. He was born in Hawaii.\"\n",
        "\n",
        "# Step 1: Tokenize the sentence\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Step 2: Part-of-Speech (POS) tagging\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "# Step 3: Named Entity Recognition (chunking)\n",
        "named_entities = nltk.ne_chunk(pos_tags)\n",
        "\n",
        "# Step 4: Print Named Entities\n",
        "print(\"Named Entities found:\")\n",
        "for chunk in named_entities:\n",
        "    if hasattr(chunk, 'label'):\n",
        "        print(f\"{chunk.label()} : {' '.join(c[0] for c in chunk)}\")"
      ],
      "metadata": {
        "id": "C27RE5vtQGch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 9-                                    -- Sentiment Analysis (Using TextBlob) --\n",
        "from textblob import TextBlob\n",
        "sentence = \"I hate you\"\n",
        "\n",
        "blob = TextBlob(sentence)\n",
        "print(blob.sentiment)"
      ],
      "metadata": {
        "id": "Fe9F1M5sAxkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 10-                                    -- Text Generation Using Transformers (GPT 2) --\n",
        "\n",
        "from transformers import pipeline, set_seed\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "print(generator(\"NLP is the future of\", truncation=True,  max_length=30, num_return_sequences=1)[0]['generated_text'])"
      ],
      "metadata": {
        "id": "bqvlpKqBAxf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Solution 11-                                    -- Text Similarity --\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "texts = [\"I love NLP\", \"I enjoy natural language processing\"]\n",
        "vec = TfidfVectorizer().fit_transform(texts)\n",
        "sim = cosine_similarity(vec)\n",
        "print(sim)\n"
      ],
      "metadata": {
        "id": "g1Hs0ge_AxdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Text Preprocessing (Beginner)\n",
        "# 🔸 Q1: Remove punctuation and lowercase a sentence\n",
        "\n",
        "# 🔸 Q2: Tokenize and remove stopwords from a sentence using NLTK\n",
        "\n",
        "#2. Lemmatization & Stemming\n",
        "# 🔸 Q3: Perform stemming using NLTK\n",
        "#    Q4: Lemmatize words using WordNetLemmatizer\n",
        "\n",
        "# 3. Part-of-Speech Tagging and NER\n",
        "# 🔸 Q5: POS Tagging using NLTK\n",
        "#    Q6: Named Entity Recognition using NLTK\n",
        "\n",
        "#4. TF-IDF & Bag of Words\n",
        "# 🔸 Q7: Convert text to TF-IDF features\n",
        "# 🔸 Q8: Build Bag of Words\n",
        "\n",
        "# 5. Word Embeddings\n",
        "# 🔸 Q9: Train Word2Vec on custom corpus\n",
        "\n",
        "# 6. Text Classification\n",
        "# 🔸 Q10: Classify movie reviews (Positive/Negative) using Multinomial Naive Bayes\n",
        "\n",
        "# 7. Text Similarity & Cosine Similarity\n",
        "# 🔸 Q11: Find cosine similarity between two texts\n",
        "\n",
        "#  8. Sentiment Analysis (Pretrained)\n",
        "# 🔸 Q12: Sentiment Analysis using TextBlob\n",
        "\n",
        "# 9. Token Classification with HuggingFace (Optional if internet works)\n",
        "# 🔸 Q13: Use pretrained NER pipeline"
      ],
      "metadata": {
        "id": "XF4L0Z-wAxbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from nltk.stem import LancasterStemmer, PorterStemmer, SnowballStemmer, RegexpStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "ueyY7YkEG2_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gNbRbAQRtUFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cHxN9sI8tUCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xTzmwTaDtT-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hhr9X9fftT7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The Importance of Saving Water. My Favorite Book and What It Taught Me\"\n",
        "tok = word_tokenize(sentence)\n",
        "tok\n",
        "\n",
        "pt = pos_tag(tok)\n",
        "pt"
      ],
      "metadata": {
        "id": "KbeF6_M6Hb2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l = LancasterStemmer()\n",
        "a = \"changing\"\n",
        "root = l(a)"
      ],
      "metadata": {
        "id": "OjEJDFgyHm_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**What is NLP?**\n",
        "\n",
        "##**NLP is a technology which is used by machine to understand, analyse and manipulate human language.**\n",
        "\n",
        "##**It is a combination of Computer Science, Artificial Intelligence and Human Language.**"
      ],
      "metadata": {
        "id": "w5GtUdqK0W1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Applications of NLP- Alexa, Siri, Google Assistance, Google Translator, To checking the span messages etc.**"
      ],
      "metadata": {
        "id": "UMBgtlnl09Gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Components of NLP->**\n",
        "                          1. NLU(Natural Language Understanding)\n",
        "                          2. NLG(Natural Language Generation)\n",
        "\n",
        "1. NLU- It is working on the probability of texts or searching that how many times that we have searched anything(e.g if we pressed \"G\" on google then it will automatically suggesting the \"Google\").\n",
        "\n",
        "2. NLG- It is basically generating something. E.g Google Translator."
      ],
      "metadata": {
        "id": "NO-GC-gx2Jzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**What are the challenges faced in NLP?**\n",
        "\n",
        "There are many challenges faced in NLP.\n",
        "\n",
        "1. Synonyms(Can't get the difference in Synos)\n",
        "2. Contextual Words(Difference in between \"Good\" and \"Better\".)\n",
        "3. Ambiguity(Hard to understand the emotion of the sentence.)\n",
        "4. Lack of research and developement.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xto_9HLh3Wm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Libraries that used in NLP- ScikitLearn, NLTK, Spacy, Tensorflow etc.**"
      ],
      "metadata": {
        "id": "j0FDRP9K9lPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The first major concern is from where we can get the data.\n",
        "\n",
        "###So we can get the data from the company itself, The data from the APIs, Web Scrapping and we can also do the survey to get the data."
      ],
      "metadata": {
        "id": "iisbcC2_3IFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the steps to create a Pipeline or the procedure to create a NLP Model.\n",
        "\n",
        "1.  DATA COLLECTION(From Google, Web, Company's Data.)\n",
        "\n",
        "2.  Data Cleaning(Removing Stop Words, Punctuation Etc.)\n",
        "\n",
        "3.  Data Preprocessing(Tokenization, Removing Digits, Creating Phrase)\n",
        "\n",
        "4.  Feature Engineering\n",
        "(Converting our data into binary numbers and removing the unimportant features.)\n",
        "\n",
        "5.  Build Model/Modeling(We can build the models by using different libraries)\n",
        "\n",
        "6.  Evaluation(Testing the models by Cross Validation, Random Cross Validation)\n",
        "\n",
        "7.  Deployment(On different website, cloud, AWS, Azure.)\n",
        "\n",
        "8.  Monetering and Updating(We can update and monitor our models in AWS or on Clouds)"
      ],
      "metadata": {
        "id": "e5KJp3_H3fuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tokenization**"
      ],
      "metadata": {
        "id": "JWjH57vU562a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**What is PUNKT**- It is a module called PUNKT available in the NLTK. NLTK (Natural Language Toolkit) is used in Python to implement programs under the domain of Natural Language Processing. It contains a variety of libraries for various purposes like text classification, parsing, stemming, tokenizing, etc.\n",
        "\n",
        "\n",
        "##In NLTK, PUNKT is an unsupervised trainable model, which means it can be trained on unlabeled data (Data that has not been tagged with information identifying its characteristics, properties, or categories is referred to as unlabeled data.)"
      ],
      "metadata": {
        "id": "84GHfPri7e3m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e32m7SRCuqNI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "x = \"The king of Mathura, a kingdom established by the Vrishni tribes. Ugrasena's son was Kamsa, who imprisoned Ugrasena and took over the kingdom\"\n",
        "w = word_tokenize(x)\n",
        "w\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Parts of Speech.**"
      ],
      "metadata": {
        "id": "7XY4Xg0s8J4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "p = pos_tag(w)    #In this section by this library we can giving some scodes to the words. E.g The=DT, King=NN, of=IN like this which is\n",
        "p"
      ],
      "metadata": {
        "id": "05d4wa516i46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "var = \"The king of Mathura, a kingdom established by the Vrishni tribes. Ugrasena's son was Kamsa, who imprisoned Ugrasena and took over the kingdom\"\n",
        "var_new = word_tokenize(var)\n",
        "var_new"
      ],
      "metadata": {
        "id": "pdFTuwbC__wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**How to remove the Stop Words?**\n"
      ],
      "metadata": {
        "id": "1mjoTIHV81Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var = \"The king of Mathura, a kingdom established by the Vrishni tribes. Ugrasena's son was Kamsa, who imprisoned Ugrasena and took over the kingdom\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "var_new = word_tokenize(var)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from string import punctuation\n",
        "\n",
        "stop = stopwords.words(\"english\")\n",
        "\n",
        "stop_word_list = list(punctuation) + stop  #By this we got the list of punctuation and all the stop words in english.\n",
        "stop_word_list"
      ],
      "metadata": {
        "id": "QzmaIz4l8XDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Writing a function to get the tokens or words without stopwords and punctuation.\n",
        "for i in var:\n",
        "  if i not in stop_word_list:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "520y7S5H9h3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Stemming and Lemmatization.**\n",
        "\n",
        "##**Stemming**\n",
        "###Stemming is a technique used to extract the base from the words by removing the affixes from them.\n",
        "\n",
        "###There are four types of Stemming algorithms-\n",
        "1. Porterstemmer\n",
        "2. Regexpstemmer(We have to provide the default parameters for removing e.g \"ing\")\n",
        "3. Snowballstemmer(It works in 15 different types of language.)\n",
        "4. Lancasterstemmer.\n",
        "\n",
        "We willl see all this with the help of examples.\n",
        "\n",
        "##**Lemmatization**\n",
        "##Lemmatization is same as Stemming but lemmatization gives us the meaningful word. The output of lemmatization is called 'Lemma'."
      ],
      "metadata": {
        "id": "S2gf2PtfFzpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer, RegexpStemmer, PorterStemmer, SnowballStemmer\n",
        "l = LancasterStemmer()\n",
        "r = RegexpStemmer(\"ing\")\n",
        "p = PorterStemmer\n",
        "s = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "hu3YqzwjFTHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(l.stem(\"changing\"))\n",
        "print(r.stem(\"changing\"))\n",
        "print(s.stem(\"changing\"))\n",
        "#print(p.stem(\"Changing\"))\n",
        "print(l.stem(\"studying\"))\n",
        "print(r.stem(\"studying\"))\n",
        "print(s.stem(\"studying\"))\n",
        "print(l.stem(\"mice\"))"
      ],
      "metadata": {
        "id": "1bP-ynSSH-hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##E.g of Lemmatization.\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "wl.lemmatize(\"mice\")\n",
        "\n",
        "wl.lemmatize(\"studying\")"
      ],
      "metadata": {
        "id": "44MsvPGlIQd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**N-grams**\n",
        "e.g An autosuggestions are coming while typing in Gboard keyboard.\n",
        "\n",
        "It is working on the probability of the text that we are searching and using the most.\n",
        "\n",
        "suppose, we types G -suggestion is -google(This is the example of Unigrams.)\n",
        "\n",
        "same as bigrams and trigrams.\n"
      ],
      "metadata": {
        "id": "6yrLkp-eMdYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"I am bunty, i am a boy, i am a good person\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "w = word_tokenize(x)\n",
        "w"
      ],
      "metadata": {
        "id": "4YYXpScHMDEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder, ngrams\n",
        "b = BigramCollocationFinder.from_words(w)\n",
        "print(b.ngram_fd)\n",
        "print(b.ngram_fd.keys())"
      ],
      "metadata": {
        "id": "HO6LJwCLOWD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = TrigramCollocationFinder.from_words(w)\n",
        "t.ngram_fd"
      ],
      "metadata": {
        "id": "U9Lymrk2O81q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "yYTwdGYUPiFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "sentence = \"The Importance of Saving Water My Favorite Book and What It Taught Me\"\n",
        "\n",
        "token = word_tokenize(sentence)\n",
        "token\n"
      ],
      "metadata": {
        "id": "rXzzAoWqPv2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "pos_tg = pos_tag(token)\n",
        "pos_tg"
      ],
      "metadata": {
        "id": "7NfMZIQdEejs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "s = \"The Importance of Saving Water. My Favorite Book and What It Taught Me\"\n",
        "sen_tok = sent_tokenize(s)\n",
        "sen_tok\n",
        "\n"
      ],
      "metadata": {
        "id": "oknNA0HRFCE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"averaged_perceptron_tagger_eng\")"
      ],
      "metadata": {
        "id": "T_KBQiWrF6Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "pos_tg = pos_tag(sen_tok)\n",
        "pos_tg"
      ],
      "metadata": {
        "id": "G8HsuCYgGDMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7angPkYxGUCQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}