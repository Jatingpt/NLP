{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJvpp2+Rq1Ex+bDpeX43Q8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jatingpt/NLP/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**What is NLP?**\n",
        "\n",
        "##**NLP is a technology which is used by machine to understand, analyse and manipulate human language.**\n",
        "\n",
        "##**It is a combination of Computer Science, Artificial Intelligence and Human Language.**"
      ],
      "metadata": {
        "id": "w5GtUdqK0W1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Applications of NLP- Alexa, Siri, Google Assistance, Google Translator, To checking the span messages etc.**"
      ],
      "metadata": {
        "id": "UMBgtlnl09Gt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Components of NLP->**\n",
        "                          1. NLU(Natural Language Understanding)\n",
        "                          2. NLG(Natural Language Generation)\n",
        "\n",
        "1. NLU- It is working on the probability of texts or searching that how many times that we have searched anything(e.g if we pressed \"G\" on google then it will automatically suggesting the \"Google\").\n",
        "\n",
        "2. NLG- It is basically generating something. E.g Google Translator."
      ],
      "metadata": {
        "id": "NO-GC-gx2Jzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**What are the challenges faced in NLP?**\n",
        "\n",
        "There are many challenges faced in NLP.\n",
        "\n",
        "1. Synonyms(Can't get the difference in Synos)\n",
        "2. Contextual Words(Difference in between \"Good\" and \"Better\".)\n",
        "3. Ambiguity(Hard to understand the emotion of the sentence.)\n",
        "4. Lack of research and developement.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xto_9HLh3Wm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Libraries that used in NLP- ScikitLearn, NLTK, Spacy, Tensorflow etc.**"
      ],
      "metadata": {
        "id": "j0FDRP9K9lPW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The first major concern is from where we can get the data.\n",
        "\n",
        "###So we can get the data from the company itself, The data from the APIs, Web Scrapping and we can also do the survey to get the data."
      ],
      "metadata": {
        "id": "iisbcC2_3IFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the steps to create a Pipeline or the procedure to create a NLP Model.\n",
        "\n",
        "1.  DATA COLLECTION(From Google, Web, Company's Data.)\n",
        "\n",
        "2.  Data Cleaning(Removing Stop Words, Punctuation Etc.)\n",
        "\n",
        "3.  Data Preprocessing(Tokenization, Removing Digits, Creating Phrase)\n",
        "\n",
        "4.  Feature Engineering\n",
        "(Converting our data into binary numbers and removing the unimportant features.)\n",
        "\n",
        "5.  Build Model/Modeling(We can build the models by using different libraries)\n",
        "\n",
        "6.  Evaluation(Testing the models by Cross Validation, Random Cross Validation)\n",
        "\n",
        "7.  Deployment(On different website, cloud, AWS, Azure.)\n",
        "\n",
        "8.  Monetering and Updating(We can update and monitor our models in AWS or on Clouds)"
      ],
      "metadata": {
        "id": "e5KJp3_H3fuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Tokenization**"
      ],
      "metadata": {
        "id": "JWjH57vU562a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**What is PUNKT**- It is a module called PUNKT available in the NLTK. NLTK (Natural Language Toolkit) is used in Python to implement programs under the domain of Natural Language Processing. It contains a variety of libraries for various purposes like text classification, parsing, stemming, tokenizing, etc.\n",
        "\n",
        "\n",
        "##In NLTK, PUNKT is an unsupervised trainable model, which means it can be trained on unlabeled data (Data that has not been tagged with information identifying its characteristics, properties, or categories is referred to as unlabeled data.)"
      ],
      "metadata": {
        "id": "84GHfPri7e3m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e32m7SRCuqNI",
        "outputId": "f442b026-b23c-4e86-dac6-a3f40b35b4fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'king',\n",
              " 'of',\n",
              " 'Mathura',\n",
              " ',',\n",
              " 'a',\n",
              " 'kingdom',\n",
              " 'established',\n",
              " 'by',\n",
              " 'the',\n",
              " 'Vrishni',\n",
              " 'tribes',\n",
              " '.',\n",
              " 'Ugrasena',\n",
              " \"'s\",\n",
              " 'son',\n",
              " 'was',\n",
              " 'Kamsa',\n",
              " ',',\n",
              " 'who',\n",
              " 'imprisoned',\n",
              " 'Ugrasena',\n",
              " 'and',\n",
              " 'took',\n",
              " 'over',\n",
              " 'the',\n",
              " 'kingdom']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "x = \"The king of Mathura, a kingdom established by the Vrishni tribes. Ugrasena's son was Kamsa, who imprisoned Ugrasena and took over the kingdom\"\n",
        "w = word_tokenize(x)\n",
        "w\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Parts of Speech.**"
      ],
      "metadata": {
        "id": "7XY4Xg0s8J4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag\n",
        "p = pos_tag(w)    #In this section by this library we can giving some scodes to the words. E.g The=DT, King=NN, of=IN like this which is\n",
        "p"
      ],
      "metadata": {
        "id": "05d4wa516i46",
        "outputId": "25c3d832-642c-4241-aef6-b55d771009af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'DT'),\n",
              " ('king', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('Mathura', 'NNP'),\n",
              " (',', ','),\n",
              " ('a', 'DT'),\n",
              " ('kingdom', 'NN'),\n",
              " ('established', 'VBN'),\n",
              " ('by', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('Vrishni', 'NNP'),\n",
              " ('tribes', 'NN'),\n",
              " ('.', '.'),\n",
              " ('Ugrasena', 'NNP'),\n",
              " (\"'s\", 'POS'),\n",
              " ('son', 'NN'),\n",
              " ('was', 'VBD'),\n",
              " ('Kamsa', 'NNP'),\n",
              " (',', ','),\n",
              " ('who', 'WP'),\n",
              " ('imprisoned', 'VBD'),\n",
              " ('Ugrasena', 'NNP'),\n",
              " ('and', 'CC'),\n",
              " ('took', 'VBD'),\n",
              " ('over', 'RP'),\n",
              " ('the', 'DT'),\n",
              " ('kingdom', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "var = \"The king of Mathura, a kingdom established by the Vrishni tribes. Ugrasena's son was Kamsa, who imprisoned Ugrasena and took over the kingdom\"\n",
        "var_new = word_tokenize(var)\n",
        "var_new"
      ],
      "metadata": {
        "id": "pdFTuwbC__wn",
        "outputId": "ac4e8d76-7e17-44cc-c5ff-d7fad10e45c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'king',\n",
              " 'of',\n",
              " 'Mathura',\n",
              " ',',\n",
              " 'a',\n",
              " 'kingdom',\n",
              " 'established',\n",
              " 'by',\n",
              " 'the',\n",
              " 'Vrishni',\n",
              " 'tribes',\n",
              " '.',\n",
              " 'Ugrasena',\n",
              " \"'s\",\n",
              " 'son',\n",
              " 'was',\n",
              " 'Kamsa',\n",
              " ',',\n",
              " 'who',\n",
              " 'imprisoned',\n",
              " 'Ugrasena',\n",
              " 'and',\n",
              " 'took',\n",
              " 'over',\n",
              " 'the',\n",
              " 'kingdom']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**How to remove the Stop Words?**\n"
      ],
      "metadata": {
        "id": "1mjoTIHV81Xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var = \"The king of Mathura, a kingdom established by the Vrishni tribes. Ugrasena's son was Kamsa, who imprisoned Ugrasena and took over the kingdom\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "var_new = word_tokenize(var)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from string import punctuation\n",
        "\n",
        "stop = stopwords.words(\"english\")\n",
        "\n",
        "stop_word_list = list(punctuation) + stop  #By this we got the list of punctuation and all the stop words in english.\n",
        "stop_word_list"
      ],
      "metadata": {
        "id": "QzmaIz4l8XDt",
        "outputId": "8f7f6332-de19-495a-ff2c-59309a8184bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['!',\n",
              " '\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '+',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '=',\n",
              " '>',\n",
              " '?',\n",
              " '@',\n",
              " '[',\n",
              " '\\\\',\n",
              " ']',\n",
              " '^',\n",
              " '_',\n",
              " '`',\n",
              " '{',\n",
              " '|',\n",
              " '}',\n",
              " '~',\n",
              " 'i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Writing a function to get the tokens or words without stopwords and punctuation.\n",
        "for i in var:\n",
        "  if i not in stop_word_list:\n",
        "    print(i)"
      ],
      "metadata": {
        "id": "520y7S5H9h3c",
        "outputId": "ba38eb10-0a13-4092-e3fe-a1f5f6bb4117",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "T\n",
            "h\n",
            "e\n",
            " \n",
            "k\n",
            "n\n",
            "g\n",
            " \n",
            "f\n",
            " \n",
            "M\n",
            "h\n",
            "u\n",
            "r\n",
            " \n",
            " \n",
            "k\n",
            "n\n",
            "g\n",
            " \n",
            "e\n",
            "b\n",
            "l\n",
            "h\n",
            "e\n",
            " \n",
            "b\n",
            " \n",
            "h\n",
            "e\n",
            " \n",
            "V\n",
            "r\n",
            "h\n",
            "n\n",
            " \n",
            "r\n",
            "b\n",
            "e\n",
            " \n",
            "U\n",
            "g\n",
            "r\n",
            "e\n",
            "n\n",
            " \n",
            "n\n",
            " \n",
            "w\n",
            " \n",
            "K\n",
            " \n",
            "w\n",
            "h\n",
            " \n",
            "p\n",
            "r\n",
            "n\n",
            "e\n",
            " \n",
            "U\n",
            "g\n",
            "r\n",
            "e\n",
            "n\n",
            " \n",
            "n\n",
            " \n",
            "k\n",
            " \n",
            "v\n",
            "e\n",
            "r\n",
            " \n",
            "h\n",
            "e\n",
            " \n",
            "k\n",
            "n\n",
            "g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Stemming and Lemmatization.**\n",
        "\n",
        "##**Stemming**\n",
        "###Stemming is a technique used to extract the base from the words by removing the affixes from them.\n",
        "\n",
        "###There are four types of Stemming algorithms-\n",
        "1. Porterstemmer\n",
        "2. Regexpstemmer(We have to provide the default parameters for removing e.g \"ing\")\n",
        "3. Snowballstemmer(It works in 15 different types of language.)\n",
        "4. Lancasterstemmer.\n",
        "\n",
        "We willl see all this with the help of examples.\n",
        "\n",
        "##**Lemmatization**\n",
        "##Lemmatization is same as Stemming but lemmatization gives us the meaningful word. The output of lemmatization is called 'Lemma'."
      ],
      "metadata": {
        "id": "S2gf2PtfFzpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer, RegexpStemmer, PorterStemmer, SnowballStemmer\n",
        "l = LancasterStemmer()\n",
        "r = RegexpStemmer(\"ing\")\n",
        "p = PorterStemmer\n",
        "s = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "hu3YqzwjFTHm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(l.stem(\"changing\"))\n",
        "print(r.stem(\"changing\"))\n",
        "print(s.stem(\"changing\"))\n",
        "#print(p.stem(\"Changing\"))\n",
        "print(l.stem(\"studying\"))\n",
        "print(r.stem(\"studying\"))\n",
        "print(s.stem(\"studying\"))\n",
        "print(l.stem(\"mice\"))"
      ],
      "metadata": {
        "id": "1bP-ynSSH-hi",
        "outputId": "52d40a86-78d6-4734-b6f3-4dbeb0d8d1e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chang\n",
            "chang\n",
            "chang\n",
            "study\n",
            "study\n",
            "studi\n",
            "mic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##E.g of Lemmatization.\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "\n",
        "wl.lemmatize(\"mice\")\n",
        "\n",
        "wl.lemmatize(\"studying\")"
      ],
      "metadata": {
        "id": "44MsvPGlIQd0",
        "outputId": "8a689922-8f77-46ea-871f-f8b948e5fc9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'studying'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**N-grams**\n",
        "e.g An autosuggestions are coming while typing in Gboard keyboard.\n",
        "\n",
        "It is working on the probability of the text that we are searching and using the most.\n",
        "\n",
        "suppose, we types G -suggestion is -google(This is the example of Unigrams.)\n",
        "\n",
        "same as bigrams and trigrams.\n"
      ],
      "metadata": {
        "id": "6yrLkp-eMdYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = \"I am bunty, i am a boy, i am a good person\"\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "w = word_tokenize(x)\n",
        "w"
      ],
      "metadata": {
        "id": "4YYXpScHMDEv",
        "outputId": "98d4b57d-35f2-4bfb-b399-f82ac85d0cc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'am',\n",
              " 'bunty',\n",
              " ',',\n",
              " 'i',\n",
              " 'am',\n",
              " 'a',\n",
              " 'boy',\n",
              " ',',\n",
              " 'i',\n",
              " 'am',\n",
              " 'a',\n",
              " 'good',\n",
              " 'person']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder, ngrams\n",
        "b = BigramCollocationFinder.from_words(w)\n",
        "print(b.ngram_fd)\n",
        "print(b.ngram_fd.keys())"
      ],
      "metadata": {
        "id": "HO6LJwCLOWD1",
        "outputId": "39f67913-4de6-49a4-a522-317a90a3e4e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 10 samples and 13 outcomes>\n",
            "dict_keys([('I', 'am'), ('am', 'bunty'), ('bunty', ','), (',', 'i'), ('i', 'am'), ('am', 'a'), ('a', 'boy'), ('boy', ','), ('a', 'good'), ('good', 'person')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = TrigramCollocationFinder.from_words(w)\n",
        "t.ngram_fd"
      ],
      "metadata": {
        "id": "U9Lymrk2O81q",
        "outputId": "45b26e3d-9ea0-4bd2-ed1f-b9939089856b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({(',', 'i', 'am'): 2, ('i', 'am', 'a'): 2, ('I', 'am', 'bunty'): 1, ('am', 'bunty', ','): 1, ('bunty', ',', 'i'): 1, ('am', 'a', 'boy'): 1, ('a', 'boy', ','): 1, ('boy', ',', 'i'): 1, ('am', 'a', 'good'): 1, ('a', 'good', 'person'): 1})"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yYTwdGYUPiFu"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rXzzAoWqPv2W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}